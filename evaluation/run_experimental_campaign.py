#!/usr/bin/env python3
"""
Simplified Experimental Campaign Runner
=====================================

Basic implementation of experimental campaign execution using
available evaluation modules.
"""

import os
import json
import time
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# Import available evaluation modules
try:
    from evaluation.systematic_resilience_evaluation import run_resilience_experiment
    from evaluation.fault_tolerant_test import FaultTolerantTest
    from evaluation.high_throughput_test import HighThroughputTest
    EVALUATION_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Evaluation modules unavailable: {e}")
    print("ðŸ’¡ Using basic demonstration mode")
    EVALUATION_MODULES_AVAILABLE = False

@dataclass
class CampaignConfig:
    """Configuration for the entire experimental campaign"""
    campaign_id: str
    output_directory: str
    phases: List[str]
    total_budget_hours: int
    max_parallel_experiments: int
    log_level: str = "INFO"
    save_intermediate_results: bool = True
    validate_data_quality: bool = True
    enable_monitoring: bool = True

@dataclass
class ExperimentalRun:
    """Single experimental run configuration"""
    run_id: str
    phase: str
    agent_type: str  # LLM, Heuristic, Hybrid
    consensus_protocol: str  # BFT, Raft, Negotiation, Weighted
    agent_count: int
    fault_rate: float
    fault_type: str
    workload_type: str
    job_arrival_rate: str
    specialization_level: float
    repetition: int
    config_file: str
    expected_duration: int  # seconds

class ExperimentalCampaignRunner:
    """Main class for executing the comprehensive experimental campaign"""
    
    def __init__(self, config: CampaignConfig):
        self.config = config
        self.setup_logging()
        self.setup_output_directories()
        self.experiment_queue = []
        self.completed_experiments = []
        self.failed_experiments = []
        self.campaign_metrics = {}
        
        # Initialize components
        self.data_validator = ExperimentalDataValidator()
        self.stats_analyzer = StatisticalAnalyzer()
        self.monitoring_system = MonitoringSystem() if config.enable_monitoring else None
        
    def setup_logging(self):
        """Configure comprehensive logging system"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = Path(self.config.output_directory) / f"campaign_{timestamp}.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config.log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Campaign {self.config.campaign_id} initialized")
        
    def setup_output_directories(self):
        """Create directory structure for experimental outputs"""
        base_dir = Path(self.config.output_directory)
        directories = [
            "raw_data", "processed_data", "analysis_results", "visualizations",
            "statistical_tests", "model_outputs", "logs", "configs", "reports"
        ]
        
        for phase in self.config.phases:
            for directory in directories:
                (base_dir / phase / directory).mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"Output directories created at {base_dir}")
        
    def generate_experiment_configurations(self) -> List[ExperimentalRun]:
        """Generate all experimental configurations based on campaign design"""
        
        # Define experimental factors
        factors = {
            'agent_type': ['LLM', 'Heuristic', 'Hybrid'],
            'consensus_protocol': ['BFT', 'Raft', 'Negotiation', 'Weighted'],
            'agent_count': [5, 10, 15, 25, 50],
            'fault_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],
            'fault_type': ['Byzantine', 'Crash', 'Network', 'Performance'],
            'workload_type': ['GPU-intensive', 'Memory-heavy', 'Compute-bound', 'I/O-heavy', 'Mixed'],
            'job_arrival_rate': ['Low', 'Medium', 'High'],
            'specialization_level': [0.0, 0.5, 1.0]
        }
        
        # Phase-specific configurations
        phase_configs = {
            'phase_1': self._generate_phase1_configs(factors),
            'phase_2': self._generate_phase2_configs(factors),
            'phase_3': self._generate_phase3_configs(factors),
            'phase_4': self._generate_phase4_configs(factors),
            'phase_5': self._generate_phase5_configs(factors)
        }
        
        all_experiments = []
        run_counter = 0
        
        for phase in self.config.phases:
            if phase in phase_configs:
                phase_experiments = phase_configs[phase]
                for exp_config in phase_experiments:
                    for rep in range(exp_config['repetitions']):
                        run_id = f"{phase}_{run_counter:04d}_{rep:02d}"
                        experiment = ExperimentalRun(
                            run_id=run_id,
                            phase=phase,
                            repetition=rep,
                            config_file=self._create_config_file(exp_config, run_id),
                            expected_duration=exp_config.get('duration', 300),
                            **{k: v for k, v in exp_config.items() 
                               if k not in ['repetitions', 'duration']}
                        )
                        all_experiments.append(experiment)
                        run_counter += 1
        
        self.logger.info(f"Generated {len(all_experiments)} experimental runs across {len(self.config.phases)} phases")
        return all_experiments
    
    def _generate_phase1_configs(self, factors) -> List[Dict]:
        """Phase 1: Baseline Establishment (120 experiments)"""
        configs = []
        
        # Full factorial: Protocol Ã— Agents Ã— Fault Rate (heuristic only)
        for protocol in factors['consensus_protocol']:
            for agents in factors['agent_count']:
                for fault_rate in factors['fault_rate']:
                    configs.append({
                        'agent_type': 'Heuristic',
                        'consensus_protocol': protocol,
                        'agent_count': agents,
                        'fault_rate': fault_rate,
                        'fault_type': 'Byzantine',  # Standard for baseline
                        'workload_type': 'Mixed',
                        'job_arrival_rate': 'Medium',
                        'specialization_level': 0.0,
                        'repetitions': 5,
                        'duration': 300
                    })
        
        return configs[:120]  # Limit to 120 as specified
    
    def _generate_phase2_configs(self, factors) -> List[Dict]:
        """Phase 2: LLM Performance Evaluation (240 experiments)"""
        configs = []
        
        # Mixed factorial: Agent Type Ã— Protocol Ã— Agents Ã— Fault Type Ã— Workload
        # Use strategic sampling instead of full factorial to manage size
        
        base_combinations = [
            ('LLM', 'BFT', 15, 'Byzantine', 'GPU-intensive'),
            ('LLM', 'Raft', 10, 'Crash', 'Compute-bound'),
            ('LLM', 'Negotiation', 25, 'Network', 'Memory-heavy'),
            ('LLM', 'Weighted', 15, 'Performance', 'Mixed'),
            ('Heuristic', 'BFT', 15, 'Byzantine', 'GPU-intensive'),
            ('Heuristic', 'Raft', 10, 'Crash', 'Compute-bound'),
            ('Hybrid', 'BFT', 20, 'Byzantine', 'Mixed'),
            ('Hybrid', 'Weighted', 15, 'Performance', 'GPU-intensive')
        ]
        
        for agent_type, protocol, agents, fault_type, workload in base_combinations:
            for fault_rate in [0.1, 0.2, 0.3]:
                for arrival_rate in ['Medium', 'High']:
                    configs.append({
                        'agent_type': agent_type,
                        'consensus_protocol': protocol,
                        'agent_count': agents,
                        'fault_rate': fault_rate,
                        'fault_type': fault_type,
                        'workload_type': workload,
                        'job_arrival_rate': arrival_rate,
                        'specialization_level': 0.5 if agent_type == 'Hybrid' else 1.0,
                        'repetitions': 3,
                        'duration': 300
                    })
        
        return configs[:240]
    
    def _generate_phase3_configs(self, factors) -> List[Dict]:
        """Phase 3: Specialization Impact Analysis (120 experiments)"""
        configs = []
        
        # Specialized factorial: Workload Ã— Specialization Ã— Protocol
        for workload in factors['workload_type']:
            for specialization in factors['specialization_level']:
                for protocol in factors['consensus_protocol']:
                    configs.append({
                        'agent_type': 'LLM',
                        'consensus_protocol': protocol,
                        'agent_count': 15,  # Fixed moderate size
                        'fault_rate': 0.2,  # Fixed moderate fault rate
                        'fault_type': 'Byzantine',
                        'workload_type': workload,
                        'job_arrival_rate': 'Medium',
                        'specialization_level': specialization,
                        'repetitions': 5,
                        'duration': 300
                    })
        
        return configs[:120]
    
    def _generate_phase4_configs(self, factors) -> List[Dict]:
        """Phase 4: Scalability and Stress Testing (96 experiments)"""
        configs = []
        
        # Stress-focused: Agent Type Ã— Protocol Ã— Agents Ã— Fault Rate
        for agent_type in ['LLM', 'Heuristic']:
            for protocol in factors['consensus_protocol']:
                for agents in [25, 50]:  # High agent counts
                    for fault_rate in [0.3, 0.4, 0.5]:  # High fault rates
                        configs.append({
                            'agent_type': agent_type,
                            'consensus_protocol': protocol,
                            'agent_count': agents,
                            'fault_rate': fault_rate,
                            'fault_type': 'Byzantine',
                            'workload_type': 'Mixed',
                            'job_arrival_rate': 'High',
                            'specialization_level': 1.0,
                            'repetitions': 2,  # Reduced due to computational cost
                            'duration': 600  # Extended duration for stress tests
                        })
        
        return configs[:96]
    
    def _generate_phase5_configs(self, factors) -> List[Dict]:
        """Phase 5: Cross-Validation and Robustness (48 experiments)"""
        
        # Selected high-impact configurations with extended runs
        high_impact_configs = [
            ('LLM', 'BFT', 15, 0.3, 'Byzantine', 'GPU-intensive', 'High', 1.0),
            ('LLM', 'Weighted', 20, 0.2, 'Performance', 'Mixed', 'Medium', 1.0),
            ('Hybrid', 'Raft', 25, 0.4, 'Crash', 'Compute-bound', 'High', 0.5),
            ('Heuristic', 'BFT', 15, 0.3, 'Byzantine', 'GPU-intensive', 'High', 0.0),
        ]
        
        configs = []
        for agent_type, protocol, agents, fault_rate, fault_type, workload, arrival, spec in high_impact_configs:
            for variation in range(12):  # 12 variations per high-impact config
                configs.append({
                    'agent_type': agent_type,
                    'consensus_protocol': protocol,
                    'agent_count': agents,
                    'fault_rate': fault_rate,
                    'fault_type': fault_type,
                    'workload_type': workload,
                    'job_arrival_rate': arrival,
                    'specialization_level': spec,
                    'repetitions': 10,  # High repetition for validation
                    'duration': 450  # Extended duration
                })
        
        return configs[:48]
    
    def _create_config_file(self, exp_config: Dict, run_id: str) -> str:
        """Create YAML configuration file for experiment"""
        config_dir = Path(self.config.output_directory) / "configs"
        config_file = config_dir / f"{run_id}_config.yaml"
        
        # Convert experiment config to executable format
        executable_config = {
            'experiment_id': run_id,
            'agent_decision_mode': exp_config['agent_type'].lower(),
            'methods': [exp_config['consensus_protocol'].lower()],
            'num_agents': exp_config['agent_count'],
            'fault_scenarios': [exp_config['fault_type'].lower()],
            'fault_intensity': self._fault_rate_to_intensity(exp_config['fault_rate']),
            'workload_profile': exp_config['workload_type'].lower().replace('-', '_'),
            'job_arrival_pattern': exp_config['job_arrival_rate'].lower(),
            'specialization_level': exp_config['specialization_level'],
            'simulation_time': exp_config['duration'],
            'output_dir': str(Path(self.config.output_directory) / exp_config.get('phase', 'unknown') / "raw_data"),
            'save_detailed_logs': True,
            'enable_monitoring': self.config.enable_monitoring
        }
        
        with open(config_file, 'w') as f:
            yaml.dump(executable_config, f, default_flow_style=False)
        
        return str(config_file)
    
    def _fault_rate_to_intensity(self, fault_rate: float) -> str:
        """Convert numeric fault rate to intensity level"""
        if fault_rate == 0.0:
            return "none"
        elif fault_rate <= 0.15:
            return "light"
        elif fault_rate <= 0.25:
            return "medium"
        elif fault_rate <= 0.35:
            return "heavy"
        else:
            return "chaos"
    
    def execute_campaign(self):
        """Execute the complete experimental campaign"""
        self.logger.info(f"Starting experimental campaign {self.config.campaign_id}")
        
        # Generate all experimental configurations
        self.experiment_queue = self.generate_experiment_configurations()
        
        # Initialize monitoring
        if self.monitoring_system:
            self.monitoring_system.initialize()
        
        start_time = time.time()
        
        try:
            # Execute experiments in parallel
            self._execute_experiments_parallel()
            
            # Analyze results
            self._analyze_campaign_results()
            
            # Generate reports
            self._generate_campaign_reports()
            
        except Exception as e:
            self.logger.error(f"Campaign execution failed: {str(e)}")
            raise
        finally:
            # Cleanup and final reporting
            self._campaign_cleanup()
            
        total_time = time.time() - start_time
        self.logger.info(f"Campaign completed in {total_time:.2f} seconds")
        
    def _execute_experiments_parallel(self):
        """Execute experiments using parallel processing"""
        
        with ThreadPoolExecutor(max_workers=self.config.max_parallel_experiments) as executor:
            # Submit all experiments
            future_to_experiment = {
                executor.submit(self._execute_single_experiment, exp): exp 
                for exp in self.experiment_queue
            }
            
            # Process completed experiments
            for future in as_completed(future_to_experiment):
                experiment = future_to_experiment[future]
                try:
                    result = future.result()
                    if result['success']:
                        self.completed_experiments.append((experiment, result))
                        self.logger.info(f"Experiment {experiment.run_id} completed successfully")
                    else:
                        self.failed_experiments.append((experiment, result))
                        self.logger.warning(f"Experiment {experiment.run_id} failed: {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    self.failed_experiments.append((experiment, {'success': False, 'error': str(e)}))
                    self.logger.error(f"Experiment {experiment.run_id} crashed: {str(e)}")
                
                # Update progress
                completed_count = len(self.completed_experiments) + len(self.failed_experiments)
                total_count = len(self.experiment_queue)
                progress = (completed_count / total_count) * 100
                self.logger.info(f"Campaign progress: {progress:.1f}% ({completed_count}/{total_count})")
    
    def _execute_single_experiment(self, experiment: ExperimentalRun) -> Dict[str, Any]:
        """Execute a single experimental run"""
        start_time = time.time()
        
        try:
            # Load configuration
            with open(experiment.config_file, 'r') as f:
                config = yaml.safe_load(f)
            
            # Determine experiment type and runner
            if config.get('fault_intensity', 'none') != 'none':
                # Use enhanced runner with fault injection
                exp_config = FaultExperimentConfig(**config)
                runner = EnhancedConsensusExperimentRunner(exp_config)
            else:
                # Use basic runner
                exp_config = ExperimentConfig(**config)
                runner = ConsensusExperimentRunner(exp_config)
            
            # Setup and execute
            runner.setup_experiment_environment()
            runner.generate_experimental_jobs()
            
            if hasattr(runner, 'inject_experimental_faults'):
                runner.inject_experimental_faults()
            
            # Run the actual experiment
            results = runner.run_experiments()
            
            # Validate results
            if self.config.validate_data_quality:
                quality_score = self.data_validator.validate_experiment_results(results)
                if quality_score < 0.8:
                    self.logger.warning(f"Low data quality score ({quality_score:.2f}) for {experiment.run_id}")
            
            execution_time = time.time() - start_time
            
            return {
                'success': True,
                'results': results,
                'execution_time': execution_time,
                'data_quality_score': quality_score if self.config.validate_data_quality else None,
                'config': config
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                'success': False,
                'error': str(e),
                'execution_time': execution_time,
                'config': config if 'config' in locals() else None
            }
    
    def _analyze_campaign_results(self):
        """Analyze results across all completed experiments"""
        self.logger.info("Starting campaign-wide analysis")
        
        if not self.completed_experiments:
            self.logger.warning("No completed experiments to analyze")
            return
        
        # Aggregate all results
        all_results = []
        for experiment, result in self.completed_experiments:
            experiment_data = {
                'run_id': experiment.run_id,
                'phase': experiment.phase,
                'agent_type': experiment.agent_type,
                'consensus_protocol': experiment.consensus_protocol,
                'agent_count': experiment.agent_count,
                'fault_rate': experiment.fault_rate,
                'fault_type': experiment.fault_type,
                'workload_type': experiment.workload_type,
                'specialization_level': experiment.specialization_level,
                'execution_time': result['execution_time'],
                'data_quality_score': result.get('data_quality_score', 0.0)
            }
            
            # Extract performance metrics from results
            if 'results' in result:
                metrics = self._extract_performance_metrics(result['results'])
                experiment_data.update(metrics)
            
            all_results.append(experiment_data)
        
        # Convert to DataFrame for analysis
        df = pd.DataFrame(all_results)
        
        # Save raw aggregated data
        output_file = Path(self.config.output_directory) / "processed_data" / "campaign_results.csv"
        df.to_csv(output_file, index=False)
        
        # Perform statistical analyses
        self.campaign_metrics = self.stats_analyzer.analyze_campaign_data(df)
        
        # Save analysis results
        analysis_file = Path(self.config.output_directory) / "analysis_results" / "statistical_analysis.json"
        with open(analysis_file, 'w') as f:
            json.dump(self.campaign_metrics, f, indent=2, default=str)
        
        self.logger.info(f"Campaign analysis completed. Results saved to {analysis_file}")
    
    def _extract_performance_metrics(self, results: Dict) -> Dict[str, float]:
        """Extract standardized performance metrics from experiment results"""
        metrics = {}
        
        # Try to extract common metrics
        if isinstance(results, dict):
            # Look for standard metric names
            metric_mappings = {
                'consensus_success_rate': ['success_rate', 'completion_rate', 'consensus_rate'],
                'convergence_time': ['convergence_time', 'consensus_time', 'avg_time'],
                'system_throughput': ['throughput', 'jobs_per_hour', 'completion_rate'],
                'fault_recovery_time': ['recovery_time', 'mean_recovery_time', 'avg_recovery'],
                'resource_utilization': ['utilization', 'resource_efficiency', 'efficiency'],
                'communication_overhead': ['messages_sent', 'overhead', 'communication_cost']
            }
            
            for standard_name, possible_names in metric_mappings.items():
                for name in possible_names:
                    if name in results:
                        metrics[standard_name] = float(results[name])
                        break
                else:
                    # Default value if not found
                    metrics[standard_name] = 0.0
        
        return metrics
    
    def _generate_campaign_reports(self):
        """Generate comprehensive campaign reports"""
        self.logger.info("Generating campaign reports")
        
        reports_dir = Path(self.config.output_directory) / "reports"
        
        # Generate summary report
        self._generate_summary_report(reports_dir)
        
        # Generate phase-specific reports
        for phase in self.config.phases:
            self._generate_phase_report(phase, reports_dir)
        
        # Generate statistical reports
        self._generate_statistical_report(reports_dir)
        
        # Generate visualizations
        self._generate_visualizations(reports_dir)
        
        self.logger.info(f"Campaign reports generated in {reports_dir}")
    
    def _generate_summary_report(self, output_dir: Path):
        """Generate executive summary report"""
        summary_file = output_dir / "campaign_summary.md"
        
        completed_count = len(self.completed_experiments)
        failed_count = len(self.failed_experiments)
        total_count = len(self.experiment_queue)
        success_rate = (completed_count / total_count) * 100 if total_count > 0 else 0
        
        total_execution_time = sum(
            result['execution_time'] for _, result in self.completed_experiments
        )
        
        summary_content = f"""# Experimental Campaign Summary
        
## Campaign Overview
- **Campaign ID**: {self.config.campaign_id}
- **Total Experiments**: {total_count}
- **Completed Successfully**: {completed_count}
- **Failed**: {failed_count}
- **Success Rate**: {success_rate:.1f}%
- **Total Execution Time**: {total_execution_time:.2f} seconds ({total_execution_time/3600:.2f} hours)

## Phase Breakdown
"""
        
        for phase in self.config.phases:
            phase_experiments = [exp for exp, _ in self.completed_experiments if exp.phase == phase]
            phase_failed = [exp for exp, _ in self.failed_experiments if exp.phase == phase]
            
            summary_content += f"""
### {phase.upper()}
- Completed: {len(phase_experiments)}
- Failed: {len(phase_failed)}
- Success Rate: {(len(phase_experiments)/(len(phase_experiments)+len(phase_failed))*100) if len(phase_experiments)+len(phase_failed) > 0 else 0:.1f}%
"""
        
        # Add key findings if available
        if hasattr(self, 'campaign_metrics') and self.campaign_metrics:
            summary_content += f"""
## Key Findings
{self._format_key_findings()}
"""
        
        with open(summary_file, 'w') as f:
            f.write(summary_content)
    
    def _format_key_findings(self) -> str:
        """Format key findings from statistical analysis"""
        findings = []
        
        if 'llm_vs_heuristic' in self.campaign_metrics:
            llm_performance = self.campaign_metrics['llm_vs_heuristic']
            findings.append(f"- LLM agents achieved {llm_performance.get('improvement', 0):.1f}% better performance than heuristic agents")
        
        if 'protocol_ranking' in self.campaign_metrics:
            best_protocol = self.campaign_metrics['protocol_ranking'][0] if self.campaign_metrics['protocol_ranking'] else "Unknown"
            findings.append(f"- Best performing protocol: {best_protocol}")
        
        if 'specialization_benefit' in self.campaign_metrics:
            spec_benefit = self.campaign_metrics['specialization_benefit']
            findings.append(f"- Agent specialization improved performance by {spec_benefit:.1f}%")
        
        return '\n'.join(findings) if findings else "Analysis in progress..."
    
    def _generate_phase_report(self, phase: str, output_dir: Path):
        """Generate detailed report for specific phase"""
        phase_file = output_dir / f"{phase}_detailed_report.json"
        
        phase_experiments = [(exp, result) for exp, result in self.completed_experiments if exp.phase == phase]
        phase_data = {
            'phase': phase,
            'experiment_count': len(phase_experiments),
            'experiments': []
        }
        
        for experiment, result in phase_experiments:
            experiment_data = {
                'run_id': experiment.run_id,
                'config': asdict(experiment),
                'results': result.get('results', {}),
                'execution_time': result['execution_time'],
                'data_quality_score': result.get('data_quality_score', None)
            }
            phase_data['experiments'].append(experiment_data)
        
        with open(phase_file, 'w') as f:
            json.dump(phase_data, f, indent=2, default=str)
    
    def _generate_statistical_report(self, output_dir: Path):
        """Generate detailed statistical analysis report"""
        stats_file = output_dir / "statistical_analysis_report.json"
        
        if hasattr(self, 'campaign_metrics'):
            with open(stats_file, 'w') as f:
                json.dump(self.campaign_metrics, f, indent=2, default=str)
    
    def _generate_visualizations(self, output_dir: Path):
        """Generate campaign visualization plots"""
        viz_dir = output_dir / "visualizations"
        viz_dir.mkdir(exist_ok=True)
        
        # This would implement various plotting functions
        # For now, create placeholder
        placeholder_file = viz_dir / "visualizations_placeholder.txt"
        with open(placeholder_file, 'w') as f:
            f.write("Visualization generation not yet implemented.\n")
            f.write("Future implementation will include:\n")
            f.write("- Performance comparison plots\n")
            f.write("- Scalability curves\n")
            f.write("- Fault tolerance analysis\n")
            f.write("- Statistical significance plots\n")
    
    def _campaign_cleanup(self):
        """Cleanup and finalization tasks"""
        self.logger.info("Performing campaign cleanup")
        
        # Archive raw data
        # Cleanup temporary files
        # Generate final manifest
        
        manifest = {
            'campaign_id': self.config.campaign_id,
            'completion_time': datetime.now().isoformat(),
            'total_experiments': len(self.experiment_queue),
            'completed_experiments': len(self.completed_experiments),
            'failed_experiments': len(self.failed_experiments),
            'output_directory': self.config.output_directory,
            'phases': self.config.phases
        }
        
        manifest_file = Path(self.config.output_directory) / "campaign_manifest.json"
        with open(manifest_file, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        self.logger.info(f"Campaign manifest saved to {manifest_file}")

class ExperimentalDataValidator:
    """Validates experimental data quality and completeness"""
    
    def validate_experiment_results(self, results: Dict) -> float:
        """Validate experiment results and return quality score (0-1)"""
        if not results:
            return 0.0
        
        quality_checks = [
            self._check_completeness(results),
            self._check_value_ranges(results),
            self._check_consistency(results),
            self._check_statistical_validity(results)
        ]
        
        return np.mean(quality_checks)
    
    def _check_completeness(self, results: Dict) -> float:
        """Check if all expected fields are present"""
        expected_fields = ['consensus_success_rate', 'convergence_time', 'system_throughput']
        present_fields = sum(1 for field in expected_fields if field in results)
        return present_fields / len(expected_fields)
    
    def _check_value_ranges(self, results: Dict) -> float:
        """Check if values are within expected ranges"""
        range_checks = {
            'consensus_success_rate': (0, 100),
            'convergence_time': (0, 1000),
            'fault_recovery_time': (0, 300)
        }
        
        valid_checks = 0
        total_checks = 0
        
        for field, (min_val, max_val) in range_checks.items():
            if field in results:
                total_checks += 1
                if min_val <= results[field] <= max_val:
                    valid_checks += 1
        
        return valid_checks / max(1, total_checks)
    
    def _check_consistency(self, results: Dict) -> float:
        """Check internal consistency of results"""
        # Placeholder for consistency checks
        return 1.0
    
    def _check_statistical_validity(self, results: Dict) -> float:
        """Check statistical validity of results"""
        # Placeholder for statistical validity checks
        return 1.0

class StatisticalAnalyzer:
    """Performs statistical analysis on campaign results"""
    
    def analyze_campaign_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Perform comprehensive statistical analysis"""
        
        analysis_results = {
            'descriptive_stats': self._descriptive_analysis(df),
            'llm_vs_heuristic': self._compare_agent_types(df),
            'protocol_effectiveness': self._analyze_protocols(df),
            'specialization_impact': self._analyze_specialization(df),
            'scalability_analysis': self._analyze_scalability(df)
        }
        
        return analysis_results
    
    def _descriptive_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate descriptive statistics"""
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        return {
            'mean': df[numeric_columns].mean().to_dict(),
            'std': df[numeric_columns].std().to_dict(),
            'count': len(df),
            'missing_data': df.isnull().sum().to_dict()
        }
    
    def _compare_agent_types(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Compare LLM vs Heuristic agent performance"""
        if 'consensus_success_rate' not in df.columns or 'agent_type' not in df.columns:
            return {'error': 'Required columns not found'}
        
        llm_performance = df[df['agent_type'] == 'LLM']['consensus_success_rate']
        heuristic_performance = df[df['agent_type'] == 'Heuristic']['consensus_success_rate']
        
        if len(llm_performance) == 0 or len(heuristic_performance) == 0:
            return {'error': 'Insufficient data for comparison'}
        
        # Perform statistical test
        statistic, p_value = stats.ttest_ind(llm_performance, heuristic_performance)
        
        # Calculate effect size (Cohen's d)
        pooled_std = np.sqrt(((len(llm_performance) - 1) * llm_performance.var() + 
                             (len(heuristic_performance) - 1) * heuristic_performance.var()) / 
                            (len(llm_performance) + len(heuristic_performance) - 2))
        cohens_d = (llm_performance.mean() - heuristic_performance.mean()) / pooled_std
        
        improvement = ((llm_performance.mean() - heuristic_performance.mean()) / 
                      heuristic_performance.mean()) * 100
        
        return {
            'llm_mean': float(llm_performance.mean()),
            'heuristic_mean': float(heuristic_performance.mean()),
            'improvement': float(improvement),
            'p_value': float(p_value),
            'cohens_d': float(cohens_d),
            'statistical_significance': p_value < 0.05
        }
    
    def _analyze_protocols(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze consensus protocol effectiveness"""
        if 'consensus_protocol' not in df.columns or 'consensus_success_rate' not in df.columns:
            return {'error': 'Required columns not found'}
        
        protocol_performance = df.groupby('consensus_protocol')['consensus_success_rate'].agg(['mean', 'std', 'count'])
        
        # Rank protocols by performance
        protocol_ranking = protocol_performance.sort_values('mean', ascending=False).index.tolist()
        
        return {
            'protocol_performance': protocol_performance.to_dict('index'),
            'protocol_ranking': protocol_ranking
        }
    
    def _analyze_specialization(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze impact of agent specialization"""
        if 'specialization_level' not in df.columns or 'consensus_success_rate' not in df.columns:
            return {'error': 'Required columns not found'}
        
        # Compare specialized vs non-specialized
        specialized = df[df['specialization_level'] > 0.5]['consensus_success_rate']
        non_specialized = df[df['specialization_level'] == 0.0]['consensus_success_rate']
        
        if len(specialized) == 0 or len(non_specialized) == 0:
            return {'error': 'Insufficient data for specialization analysis'}
        
        improvement = ((specialized.mean() - non_specialized.mean()) / 
                      non_specialized.mean()) * 100
        
        return {
            'specialized_mean': float(specialized.mean()),
            'non_specialized_mean': float(non_specialized.mean()),
            'specialization_benefit': float(improvement)
        }
    
    def _analyze_scalability(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze system scalability"""
        if 'agent_count' not in df.columns or 'consensus_success_rate' not in df.columns:
            return {'error': 'Required columns not found'}
        
        # Analyze performance vs agent count
        scalability_data = df.groupby('agent_count')['consensus_success_rate'].agg(['mean', 'std'])
        
        # Calculate performance degradation
        agent_counts = sorted(scalability_data.index)
        performance_values = [scalability_data.loc[count, 'mean'] for count in agent_counts]
        
        # Find performance threshold (where performance drops below 80% of max)
        max_performance = max(performance_values)
        threshold = max_performance * 0.8
        
        scalability_limit = None
        for i, (count, perf) in enumerate(zip(agent_counts, performance_values)):
            if perf < threshold:
                scalability_limit = agent_counts[i-1] if i > 0 else agent_counts[0]
                break
        
        return {
            'scalability_data': scalability_data.to_dict('index'),
            'max_performance': float(max_performance),
            'scalability_limit': scalability_limit,
            'performance_threshold': float(threshold)
        }

class MonitoringSystem:
    """Real-time monitoring system for experiments"""
    
    def __init__(self):
        self.metrics = {}
        self.alerts = []
    
    def initialize(self):
        """Initialize monitoring system"""
        pass
    
    def record_metric(self, metric_name: str, value: float):
        """Record a metric value"""
        if metric_name not in self.metrics:
            self.metrics[metric_name] = []
        self.metrics[metric_name].append((time.time(), value))
    
    def check_alerts(self):
        """Check for alert conditions"""
        pass

def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(description="Execute LLM Consensus Experimental Campaign")
    parser.add_argument('--config', '-c', required=True, help='Campaign configuration file')
    parser.add_argument('--phases', nargs='+', default=['phase_1', 'phase_2', 'phase_3', 'phase_4', 'phase_5'],
                       help='Phases to execute')
    parser.add_argument('--output', '-o', default='./experimental_campaign_results',
                       help='Output directory')
    parser.add_argument('--parallel', '-p', type=int, default=4,
                       help='Maximum parallel experiments')
    parser.add_argument('--dry-run', action='store_true',
                       help='Generate configurations but do not execute')
    
    args = parser.parse_args()
    
    # Load campaign configuration
    if args.config.endswith('.yaml') or args.config.endswith('.yml'):
        with open(args.config, 'r') as f:
            config_data = yaml.safe_load(f)
    else:
        with open(args.config, 'r') as f:
            config_data = json.load(f)
    
    # Create campaign configuration
    campaign_config = CampaignConfig(
        campaign_id=config_data.get('campaign_id', f'campaign_{int(time.time())}'),
        output_directory=args.output,
        phases=args.phases,
        total_budget_hours=config_data.get('total_budget_hours', 1000),
        max_parallel_experiments=args.parallel,
        log_level=config_data.get('log_level', 'INFO'),
        save_intermediate_results=config_data.get('save_intermediate_results', True),
        validate_data_quality=config_data.get('validate_data_quality', True),
        enable_monitoring=config_data.get('enable_monitoring', True)
    )
    
    # Initialize and run campaign
    campaign_runner = ExperimentalCampaignRunner(campaign_config)
    
    if args.dry_run:
        print(f"DRY RUN: Generating experiment configurations...")
        experiments = campaign_runner.generate_experiment_configurations()
        print(f"Generated {len(experiments)} experimental configurations")
        
        # Show sample configurations
        for i, exp in enumerate(experiments[:5]):
            print(f"\nSample {i+1}: {exp.run_id}")
            print(f"  Phase: {exp.phase}")
            print(f"  Agent Type: {exp.agent_type}")
            print(f"  Protocol: {exp.consensus_protocol}")
            print(f"  Agents: {exp.agent_count}")
            print(f"  Fault Rate: {exp.fault_rate}")
    else:
        campaign_runner.execute_campaign()

if __name__ == '__main__':
    main()
