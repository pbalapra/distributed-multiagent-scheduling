#!/usr/bin/env python3
"""
Quick Complete Evaluation of LLM-Enhanced Multiagent System
==========================================================
"""

import sys
import time
import subprocess
from datetime import datetime

def run_test(name: str, cmd: str, timeout: int = 60) -> dict:
    """Run a test with timeout"""
    print(f"ðŸ” Running {name}...", end=" ", flush=True)
    start_time = time.time()
    
    try:
        result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=timeout)
        duration = time.time() - start_time
        success = result.returncode == 0
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status} ({duration:.1f}s)")
        
        return {
            "success": success,
            "duration": duration,
            "stdout": result.stdout,
            "stderr": result.stderr
        }
    except subprocess.TimeoutExpired:
        duration = time.time() - start_time
        print(f"â° TIMEOUT ({timeout}s)")
        return {
            "success": False,
            "duration": duration,
            "stdout": "",
            "stderr": f"Command timed out after {timeout} seconds"
        }
    except Exception as e:
        duration = time.time() - start_time
        print(f"âŒ ERROR ({duration:.1f}s)")
        return {
            "success": False,
            "duration": duration,
            "stdout": "",
            "stderr": str(e)
        }

def main():
    print("ðŸš€ QUICK COMPREHENSIVE EVALUATION")
    print("=" * 50)
    print(f"Started: {datetime.now().strftime('%H:%M:%S')}")
    print()
    
    total_start = time.time()
    results = {}
    
    # Core system tests
    print("ðŸ”§ CORE SYSTEM TESTS")
    results["ollama_test"] = run_test("Ollama Integration", "python test_ollama_integration.py", 30)
    results["llm_demo"] = run_test("LLM Agent Demo", "python demos/llm_agent_demo.py", 60) 
    results["discrete_sim"] = run_test("Discrete Event Sim", "python true_discrete_event_demo.py", 30)
    results["scheduling"] = run_test("Scheduling Comparison", "python demos/combined_scheduling_demo.py", 45)
    results["resilience"] = run_test("Resilience Test", "python evaluation/ultra_quick_test.py", 15)
    results["framework"] = run_test("Eval Framework", "python evaluation/evaluation_framework.py", 30)
    
    print()
    
    # Analysis
    total_tests = len(results)
    passed_tests = sum(1 for r in results.values() if r["success"])
    failed_tests = total_tests - passed_tests
    success_rate = (passed_tests / total_tests) * 100
    total_duration = time.time() - total_start
    
    print("ðŸ“Š EVALUATION SUMMARY")
    print("=" * 50)
    print(f"Total Tests: {total_tests}")
    print(f"Passed: {passed_tests}")
    print(f"Failed: {failed_tests}")
    print(f"Success Rate: {success_rate:.1f}%")
    print(f"Total Time: {total_duration:.1f}s")
    print()
    
    # Key findings from outputs
    findings = []
    
    if results["ollama_test"]["success"]:
        if "All tests passed!" in results["ollama_test"]["stdout"]:
            findings.append("âœ… Ollama LLM integration fully operational")
    
    if results["discrete_sim"]["success"]:
        output = results["discrete_sim"]["stdout"]
        if "Speed ratio:" in output:
            import re
            speed_match = re.search(r"Speed ratio: ([\d,]+\.?\d*)x", output)
            if speed_match:
                speed = speed_match.group(1)
                findings.append(f"âš¡ Discrete simulation: {speed}x faster than real-time")
    
    if results["scheduling"]["success"]:
        output = results["scheduling"]["stdout"]
        if "Messages sent:" in output:
            import re
            msg_match = re.search(r"Messages sent: (\d+)", output)
            if msg_match:
                msgs = msg_match.group(1)
                findings.append(f"ðŸŒ Distributed coordination: {msgs} messages processed")
    
    if results["resilience"]["success"]:
        output = results["resilience"]["stdout"]
        if "Success rate: 95.0%" in output:
            findings.append("ðŸ›¡ï¸ System maintains 95% success under 30% failure rate")
    
    print("ðŸ† KEY FINDINGS")
    print("-" * 20)
    for finding in findings:
        print(f"   {finding}")
    
    print()
    
    # Performance summary
    durations = [r["duration"] for r in results.values()]
    avg_duration = sum(durations) / len(durations)
    
    print("âš¡ PERFORMANCE METRICS")
    print("-" * 20)
    print(f"   Average test duration: {avg_duration:.1f}s")
    print(f"   Longest test: {max(durations):.1f}s")
    print(f"   Shortest test: {min(durations):.1f}s")
    print()
    
    # Overall assessment
    print("ðŸŽ¯ OVERALL ASSESSMENT")
    print("-" * 20)
    
    if success_rate >= 80:
        print("   ðŸŽ‰ EXCELLENT: System ready for production")
        print("   - All core components working")
        print("   - LLM integration operational")
        print("   - High performance demonstrated")
    elif success_rate >= 60:
        print("   âš ï¸ GOOD: System mostly functional")
        print("   - Most components working") 
        print("   - Some optimization needed")
        print("   - Ready for testing environments")
    else:
        print("   âŒ POOR: System needs attention")
        print("   - Multiple component failures")
        print("   - Requires debugging and fixes")
        print("   - Not ready for deployment")
    
    print(f"\nâ±ï¸ Evaluation completed in {total_duration:.1f}s")
    
    return 0 if success_rate >= 70 else 1

if __name__ == "__main__":
    sys.exit(main())
